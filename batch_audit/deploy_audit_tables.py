from pyspark.sql import SparkSession
from datetime import datetime
import time
spark= SparkSession.builder.getOrCreate()

def deploy_app_tables(schema = 'default'):
    import datetime
    spark.sql(f"""
    CREATE TABLE {schema}.application_reference (  
    app_id BIGINT generated BY default as IDENTITY (start with 1),
    application_short_name STRING,  
    source_category STRING,  
    line_of_business STRING,  
    insurance_type STRING,  
    application_name STRING,  
    application_description STRING,
    validation_summary_option STRING, 
    reconcilation_summary_option STRING,
    active_status STRING    
    ) USING DELTA
    TBLPROPERTIES('delta.feature.allowColumnDefaults' = 'supported'); """) 
 
    spark.sql(f"""ALTER TABLE {schema}.application_reference ALTER COLUMN active_status SET DEFAULT 'Y';""")
    spark.sql(f"""ALTER TABLE {schema}.application_reference ALTER COLUMN validation_summary_option SET DEFAULT '{{}}';""")
    spark.sql(f"""ALTER TABLE {schema}.application_reference ALTER COLUMN reconcilation_summary_option SET DEFAULT '{{}}';""")

    spark.sql(f"""CREATE TABLE {schema}.application_task_reference (  
    app_id bigint,  
    table_load_name STRING,  
    load_sequence_nbr INT,  
    data_layer STRING,  
    source_type STRING,
    source_format STRING,
    source_table STRING,  
    source_option STRING,  
    dependent_tables_list_options STRING,  
    transformation_flag STRING,  
    quality_flag STRING,  
    validation_flag STRING,  
    target_type STRING,
    target_format STRING,
    target_table STRING,  
    target_option STRING,  
    reject_table STRING,  
    reject_option STRING,
    validation_log_table STRING,  
    validation_log_option STRING,
    active_status STRING
    ) USING DELTA
    TBLPROPERTIES('delta.feature.allowColumnDefaults' = 'supported');""")
 
    spark.sql(f"""ALTER TABLE {schema}.application_task_reference ALTER COLUMN active_status SET DEFAULT 'Y';""")
    spark.sql(f"""ALTER TABLE {schema}.application_task_reference ALTER COLUMN reject_option SET DEFAULT '{{}}';""")
    spark.sql(f"""ALTER TABLE {schema}.application_task_reference ALTER COLUMN validation_log_option SET DEFAULT '{{}}';""")
    spark.sql(f"""ALTER TABLE {schema}.application_task_reference ALTER COLUMN target_option SET DEFAULT '{{}}';""")

    spark.sql(f"""CREATE TABLE {schema}.application_transformation_reference (  
    app_id bigint,    
    table_load_name STRING,  
    load_sequence_nbr INT,  
    rule_sequence_nbr int,    
    rule_parser STRING,
    rule_override STRING,
    transformation_name STRING,  
    transformation_input STRING,  
    transformation_output STRING,
    active_status string
    ) USING DELTA
    TBLPROPERTIES('delta.feature.allowColumnDefaults' = 'supported');""")
 
    spark.sql(f"""ALTER TABLE {schema}.application_transformation_reference ALTER COLUMN active_status SET DEFAULT 'Y';""")

    spark.sql(f"""CREATE TABLE {schema}.application_quality_reference (  
    app_id bigint,    
    table_load_name STRING,  
    load_sequence_nbr INT,  
    rule_sequence_nbr int,    
    rule_parser STRING,
    rule_override STRING,
    data_field string,
    validation_name STRING,  
    validation_input STRING,  
    validation_output STRING,
    exception_handling string,
    attribute_value string, 
    active_status string
    ) USING DELTA
    TBLPROPERTIES('delta.feature.allowColumnDefaults' = 'supported');""")
 
    spark.sql(f"""ALTER TABLE {schema}.application_quality_reference ALTER COLUMN active_status SET DEFAULT 'Y';""")
 
    spark.sql(f"""CREATE TABLE {schema}.batch_history (  
            batch_id BIGINT generated BY default as IDENTITY (start with 1000),
            application_short_name STRING,  
            source_category STRING,  
            batch_start_timestamp_utc TIMESTAMP,  
            batch_end_timestamp_utc TIMESTAMP,  
            batch_status STRING,
            batch_message STRING  
        )  
        USING DELTA
        TBLPROPERTIES ('delta.enableDeletionVectors' = true);""")
 
    spark.sql(f"""CREATE TABLE {schema}.task_history (  
    task_id  STRING NOT NULL,  
    batch_id  bigint NOT NULL,
    Table_load_name STRING,  
    load_sequence_nbr INT,  
    data_layer STRING,  
    extract_start_timestamp_utc TIMESTAMP,
    extract_end_timestamp_utc TIMESTAMP,
    task_start_timestamp_utc TIMESTAMP,  
    task_end_timestamp_utc TIMESTAMP,  
    task_status STRING ,
    task_message STRING 
    )  
    USING DELTA
    TBLPROPERTIES ('delta.enableDeletionVectors' = true); """)
 

def deploy_tables():
    import argparse
    parser = argparse.ArgumentParser(description='Process Data Pipeline.')

    parser.add_argument( '-db','--schema', dest='schema', required = True, default='edw_shared', 
                        help='pass schema for dp run')

    args = parser.parse_args()
    deploy_app_tables(args.schema)