{
	"name": "adf_pl_horse_history_raw_load",
	"properties": {
		"activities": [
			{
				"name": "adf_pl_params",
				"type": "Lookup",
				"dependsOn": [],
				"policy": {
					"timeout": "0.12:00:00",
					"retry": 0,
					"retryIntervalInSeconds": 30,
					"secureOutput": false,
					"secureInput": false
				},
				"userProperties": [],
				"typeProperties": {
					"source": {
						"type": "DelimitedTextSource",
						"storeSettings": {
							"type": "AzureBlobStorageReadSettings",
							"recursive": true,
							"enablePartitionDiscovery": false
						},
						"formatSettings": {
							"type": "DelimitedTextReadSettings"
						}
					},
					"dataset": {
						"referenceName": "E2_HORSE_PARAMS",
						"type": "DatasetReference"
					},
					"firstRowOnly": false
				}
			},
			{
				"name": "History data load from Magic to raw",
				"description": "History data load from Magic to raw ",
				"type": "DatabricksSparkPython",
				"dependsOn": [
					{
						"activity": "adf_pl_params",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"policy": {
					"timeout": "0.12:00:00",
					"retry": 0,
					"retryIntervalInSeconds": 30,
					"secureOutput": false,
					"secureInput": false
				},
				"userProperties": [],
				"typeProperties": {
					"pythonFile": "dbfs:/FileStore/main_adf_setup.py",
					"parameters": [
						"stage_load",
						"@activity('adf_pl_params').output.value[0].param_value",
						"@activity('adf_pl_params').output.value[1].param_value",
						"@activity('adf_pl_params').output.value[2].param_value",
						"@activity('adf_pl_params').output.value[5].param_value",
						"@activity('adf_pl_params').output.value[6].param_value",
						"RAW_HISTORY_XML",
						"raw",
						"@activity('adf_pl_params').output.value[3].param_value"
					],
					"libraries": [
						{
							"whl": "dbfs:/FileStore/jars/cyberclouddatapipeline/industryclouddatapipeline-0.1.5-py3-none-any.whl"
						}
					]
				},
				"linkedServiceName": {
					"referenceName": "AzureDatabricks",
					"type": "LinkedServiceReference"
				}
			},
			{
				"name": "notify_failure",
				"type": "DatabricksSparkPython",
				"dependsOn": [
					{
						"activity": "History data load from Magic to raw",
						"dependencyConditions": [
							"Failed"
						]
					}
				],
				"policy": {
					"timeout": "0.12:00:00",
					"retry": 0,
					"retryIntervalInSeconds": 30,
					"secureOutput": false,
					"secureInput": false
				},
				"userProperties": [],
				"typeProperties": {
					"pythonFile": "dbfs:/FileStore/main_adf_setup.py",
					"parameters": [
						"notify",
						"@activity('adf_pl_params').output.value[0].param_value",
						"@activity('adf_pl_params').output.value[1].param_value",
						"@activity('adf_pl_params').output.value[2].param_value",
						"failed",
						"@concat(activity('adf_pl_params').output.value[0].param_value,' ', activity('adf_pl_params').output.value[1].param_value,' Job Failed due to 0 records available for the date range from source.<br> Clearing all audit entries')"
					],
					"libraries": [
						{
							"whl": "dbfs:/FileStore/jars/cyberclouddatapipeline/industryclouddatapipeline-0.1.5-py3-none-any.whl"
						}
					]
				},
				"linkedServiceName": {
					"referenceName": "AzureDatabricks",
					"type": "LinkedServiceReference"
				}
			},
			{
				"name": "Clear Audit Entry",
				"type": "DatabricksSparkPython",
				"dependsOn": [
					{
						"activity": "notify_failure",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"policy": {
					"timeout": "0.12:00:00",
					"retry": 0,
					"retryIntervalInSeconds": 30,
					"secureOutput": false,
					"secureInput": false
				},
				"userProperties": [],
				"typeProperties": {
					"pythonFile": "dbfs:/FileStore/main_adf_setup.py",
					"parameters": [
						"clear_audit",
						"@activity('adf_pl_params').output.value[0].param_value",
						"@activity('adf_pl_params').output.value[1].param_value",
						"@activity('adf_pl_params').output.value[2].param_value",
						""
					],
					"libraries": [
						{
							"whl": "dbfs:/FileStore/jars/cyberclouddatapipeline/industryclouddatapipeline-0.1.5-py3-none-any.whl"
						}
					]
				},
				"linkedServiceName": {
					"referenceName": "AzureDatabricks",
					"type": "LinkedServiceReference"
				}
			},
			{
				"name": "Fail",
				"type": "Fail",
				"dependsOn": [
					{
						"activity": "Clear Audit Entry",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"userProperties": [],
				"typeProperties": {
					"message": "\"No record to processed\"",
					"errorCode": "1"
				}
			}
		],
		"folder": {
			"name": "HORSE"
		},
		"annotations": [],
		"lastPublishTime": "2024-05-16T09:54:08Z"
	},
	"type": "Microsoft.DataFactory/factories/pipelines"
}